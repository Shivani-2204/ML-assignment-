import numpy as np
import pandas as pd

# Load data
X = pd.read_csv('logisticX.csv').values
y = pd.read_csv('logisticY.csv').values.flatten()

# Add intercept term
X = np.c_[np.ones(X.shape[0]), X]

# Initialize parameters
weights = np.zeros(X.shape[1])
learning_rate = 0.1
iterations = 1000

# Define sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define cost function
def compute_cost(X, y, weights):
    m = len(y)
    predictions = sigmoid(np.dot(X, weights))
    epsilon = 1e-15
    cost = -1/m * np.sum(y * np.log(predictions + epsilon) + (1 - y) * np.log(1 - predictions + epsilon))
    return cost

# Gradient Descent
def gradient_descent(X, y, weights, learning_rate, iterations):
    m = len(y)
    cost_history = []
    for _ in range(iterations):
        predictions = sigmoid(np.dot(X, weights))
        gradient = np.dot(X.T, (predictions - y)) / m
        weights -= learning_rate * gradient
        cost = compute_cost(X, y, weights)
        cost_history.append(cost)
    return weights, cost_history

# Train the logistic regression model
final_weights, cost_history = gradient_descent(X, y, weights, learning_rate, iterations)

# Final results
final_cost = cost_history[-1]
print(f"Final Cost Function Value: {final_cost}")
print(f"Final Weights (Coefficients): {final_weights}")
 57 changes: 57 additions & 0 deletions57  
Assignment 2/2.py
Original file line number	Diff line number	Diff line change
@@ -0,0 +1,57 @@
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the datasets
# Replace 'X.csv' and 'y.csv' with the actual file paths if different
X = pd.read_csv('logisticX.csv').values  # Independent variables
y = pd.read_csv('logisticY.csv').values.flatten()  # Dependent variable (flatten to 1D array)

# Step 2: Add intercept term
X = np.c_[np.ones(X.shape[0]), X]  # Add a column of ones for the intercept

# Step 3: Initialize parameters
weights = np.zeros(X.shape[1])  # Initialize weights to zeros
learning_rate = 0.1  # Learning rate
iterations = 1000  # Number of iterations

# Define sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define cost function
def compute_cost(X, y, weights):
    m = len(y)
    predictions = sigmoid(np.dot(X, weights))
    epsilon = 1e-15  # To prevent log(0)
    cost = -1/m * np.sum(y * np.log(predictions + epsilon) + (1 - y) * np.log(1 - predictions + epsilon))
    return cost

# Gradient Descent Function
def gradient_descent(X, y, weights, learning_rate, iterations):
    m = len(y)
    cost_history = []
    for _ in range(iterations):
        predictions = sigmoid(np.dot(X, weights))
        gradient = np.dot(X.T, (predictions - y)) / m
        weights -= learning_rate * gradient
        cost = compute_cost(X, y, weights)
        cost_history.append(cost)
    return weights, cost_history

# Train logistic regression model
final_weights, cost_history = gradient_descent(X, y, weights, learning_rate, iterations)

# Step 4: Plot cost vs. iterations (first 50 iterations)
plt.figure(figsize=(8, 6))
plt.plot(range(1, 51), cost_history[:50], marker='o', linestyle='-', color='b')
plt.title("Cost Function vs. Iterations (First 50 Iterations)", fontsize=14)
plt.xlabel("Iteration", fontsize=12)
plt.ylabel("Cost Function Value", fontsize=12)
plt.grid(True)
plt.show()

# Step 5: Print final cost and weights
final_cost = cost_history[-1]
print(f"Final Cost Function Value: {final_cost}")
print(f"Final Weights (Coefficients): {final_weights}")
 73 changes: 73 additions & 0 deletions73  
Assignment 2/3.py
Original file line number	Diff line number	Diff line change
@@ -0,0 +1,73 @@
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the datasets
# Replace 'X.csv' and 'y.csv' with the actual file paths if different
X = pd.read_csv('logisticX.csv').values  # Independent variables
y = pd.read_csv('logisticY.csv').values.flatten()  # Dependent variable (flatten to 1D array)

# Step 2: Add intercept term
X = np.c_[np.ones(X.shape[0]), X]  # Add a column of ones for the intercept

# Step 3: Initialize parameters
weights = np.zeros(X.shape[1])  # Initialize weights to zeros
learning_rate = 0.1  # Learning rate
iterations = 1000  # Number of iterations

# Define sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define cost function
def compute_cost(X, y, weights):
    m = len(y)
    predictions = sigmoid(np.dot(X, weights))
    epsilon = 1e-15  # To prevent log(0)
    cost = -1/m * np.sum(y * np.log(predictions + epsilon) + (1 - y) * np.log(1 - predictions + epsilon))
    return cost

# Gradient Descent Function
def gradient_descent(X, y, weights, learning_rate, iterations):
    m = len(y)
    cost_history = []
    for _ in range(iterations):
        predictions = sigmoid(np.dot(X, weights))
        gradient = np.dot(X.T, (predictions - y)) / m
        weights -= learning_rate * gradient
        cost = compute_cost(X, y, weights)
        cost_history.append(cost)
    return weights, cost_history

# Train logistic regression model
final_weights, cost_history = gradient_descent(X, y, weights, learning_rate, iterations)

# Step 4: Plot the dataset and decision boundary
plt.figure(figsize=(8, 6))

# Separate points by class
class_0 = X[y == 0]
class_1 = X[y == 1]

# Plot class 0
plt.scatter(class_0[:, 1], class_0[:, 2], color='red', label='Class 0')
# Plot class 1
plt.scatter(class_1[:, 1], class_1[:, 2], color='blue', label='Class 1')

# Plot decision boundary
x_values = np.array([min(X[:, 1]), max(X[:, 1])])
y_values = -(final_weights[0] + final_weights[1] * x_values) / final_weights[2]
plt.plot(x_values, y_values, color='green', label='Decision Boundary')

# Aesthetics
plt.title("Dataset with Decision Boundary", fontsize=14)
plt.xlabel("Feature 1", fontsize=12)
plt.ylabel("Feature 2", fontsize=12)
plt.legend()
plt.grid(True)
plt.show()

# Step 5: Print final cost and weights
final_cost = cost_history[-1]
print(f"Final Cost Function Value: {final_cost}")
print(f"Final Weights (Coefficients): {final_weights}")
 81 changes: 81 additions & 0 deletions81  
Assignment 2/4.py
Original file line number	Diff line number	Diff line change
@@ -0,0 +1,81 @@
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the datasets
# Replace 'X.csv' and 'y.csv' with the actual file paths if different
X = pd.read_csv('logisticX.csv').values  # Independent variables
y = pd.read_csv('logisticY.csv').values.flatten()  # Dependent variable (flatten to 1D array)

# Step 2: Introduce two new independent variables (squares of the original variables)
X_new = np.c_[X, X[:, 0]**2, X[:, 1]**2]  # Add squared features
X_new = np.c_[np.ones(X_new.shape[0]), X_new]  # Add a column of ones for the intercept

# Step 3: Initialize parameters
weights = np.zeros(X_new.shape[1])  # Initialize weights to zeros
learning_rate = 0.1  # Learning rate
iterations = 1000  # Number of iterations

# Define sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define cost function
def compute_cost(X, y, weights):
    m = len(y)
    predictions = sigmoid(np.dot(X, weights))
    epsilon = 1e-15  # To prevent log(0)
    cost = -1/m * np.sum(y * np.log(predictions + epsilon) + (1 - y) * np.log(1 - predictions + epsilon))
    return cost

# Gradient Descent Function
def gradient_descent(X, y, weights, learning_rate, iterations):
    m = len(y)
    cost_history = []
    for _ in range(iterations):
        predictions = sigmoid(np.dot(X, weights))
        gradient = np.dot(X.T, (predictions - y)) / m
        weights -= learning_rate * gradient
        cost = compute_cost(X, y, weights)
        cost_history.append(cost)
    return weights, cost_history

# Train logistic regression model on the new dataset
final_weights, cost_history = gradient_descent(X_new, y, weights, learning_rate, iterations)

# Step 4: Plot the dataset and decision boundary
plt.figure(figsize=(8, 6))

# Separate points by class
class_0 = X[y == 0]
class_1 = X[y == 1]

# Plot class 0
plt.scatter(class_0[:, 0], class_0[:, 1], color='red', label='Class 0')
# Plot class 1
plt.scatter(class_1[:, 0], class_1[:, 1], color='blue', label='Class 1')

# Plot decision boundary
x_min, x_max = X[:, 0].min(), X[:, 0].max()
y_min, y_max = X[:, 1].min(), X[:, 1].max()

xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))
grid = np.c_[xx.ravel(), yy.ravel(), xx.ravel()**2, yy.ravel()**2]
grid = np.c_[np.ones(grid.shape[0]), grid]  # Add intercept term
predictions = sigmoid(np.dot(grid, final_weights)).reshape(xx.shape)

# Contour plot for the decision boundary
plt.contour(xx, yy, predictions, levels=[0.5], colors='green', linewidths=2, label='Decision Boundary')

# Aesthetics
plt.title("Dataset with New Features and Decision Boundary", fontsize=14)
plt.xlabel("Feature 1", fontsize=12)
plt.ylabel("Feature 2", fontsize=12)
plt.legend()
plt.grid(True)
plt.show()

# Step 5: Print final cost and weights
final_cost = cost_history[-1]
print(f"Final Cost Function Value: {final_cost}")
print(f"Final Weights (Coefficients): {final_weights}")
 90 changes: 90 additions & 0 deletions90  
Assignment 2/5.py
Original file line number	Diff line number	Diff line change
@@ -0,0 +1,90 @@
import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# Step 1: Load the datasets
X = pd.read_csv('logisticX.csv').values  # Independent variables
y = pd.read_csv('logisticY.csv').values.flatten()  # Dependent variable (flatten to 1D array)

# Step 2: Add intercept term
X = np.c_[np.ones(X.shape[0]), X]  # Add a column of ones for the intercept

# Step 3: Initialize parameters
weights = np.zeros(X.shape[1])  # Initialize weights to zeros
learning_rate = 0.1  # Learning rate
iterations = 1000  # Number of iterations

# Define sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define cost function
def compute_cost(X, y, weights):
    m = len(y)
    predictions = sigmoid(np.dot(X, weights))
    epsilon = 1e-15  # To prevent log(0)
    cost = -1/m * np.sum(y * np.log(predictions + epsilon) + (1 - y) * np.log(1 - predictions + epsilon))
    return cost

# Gradient Descent Function
def gradient_descent(X, y, weights, learning_rate, iterations):
    m = len(y)
    cost_history = []
    for _ in range(iterations):
        predictions = sigmoid(np.dot(X, weights))
        gradient = np.dot(X.T, (predictions - y)) / m
        weights -= learning_rate * gradient
        cost = compute_cost(X, y, weights)
        cost_history.append(cost)
    return weights, cost_history

# Train logistic regression model
final_weights, cost_history = gradient_descent(X, y, weights, learning_rate, iterations)

# Step 4: Make predictions
def predict(X, weights):
    probabilities = sigmoid(np.dot(X, weights))
    return [1 if prob >= 0.5 else 0 for prob in probabilities]

y_pred = predict(X, final_weights)

# Step 5: Calculate metrics
conf_matrix = confusion_matrix(y, y_pred)
accuracy = accuracy_score(y, y_pred)
precision = precision_score(y, y_pred)
recall = recall_score(y, y_pred)
f1 = f1_score(y, y_pred)

# Step 6: Print results
print("Confusion Matrix:")
print(conf_matrix)
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# Step 7: Optional - Plot the dataset and decision boundary for visualization
plt.figure(figsize=(8, 6))

# Separate points by class
class_0 = X[y == 0]
class_1 = X[y == 1]

# Plot class 0
plt.scatter(class_0[:, 1], class_0[:, 2], color='red', label='Class 0')
# Plot class 1
plt.scatter(class_1[:, 1], class_1[:, 2], color='blue', label='Class 1')

# Plot decision boundary
x_values = np.array([min(X[:, 1]), max(X[:, 1])])
y_values = -(final_weights[0] + final_weights[1] * x_values) / final_weights[2]
plt.plot(x_values, y_values, color='green', label='Decision Boundary')

# Aesthetics
plt.title("Dataset with Decision Boundary", fontsize=14)
plt.xlabel("Feature 1", fontsize=12)
plt.ylabel("Feature 2", fontsize=12)
plt.legend()
plt.grid(True)
plt.show()
